## AUC-ROC Curve:
https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/

## Discrimination: 
This refers to how well the model can distinguish between different outcomes. In this context, the discrimination assessment involves calculating the Area Under the Curve (AUC) of the receiver operating characteristic (ROC) curve. The ROC curve shows the trade-off between the true positive rate and the false positive rate across different classification thresholds.
## Calibration: 
Calibration evaluates how well the predicted probabilities from the model match the observed outcomes. The Hosmer-Lemeshow goodness-of-fit statistic is commonly used for this purpose. It involves dividing the data into groups based on the predicted probabilities and comparing the observed and expected outcomes within those groups.
